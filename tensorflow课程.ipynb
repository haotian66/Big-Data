{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/fire_theft.xls'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-346b449b7660>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mDATA_FILE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"data/fire_theft.xls\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Step 1: read in data from the .xls file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m \u001b[1;33m.\u001b[0m \u001b[0mopen_workbook\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mDATA_FILE\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mencoding_override\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0msheet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbook\u001b[0m \u001b[1;33m.\u001b[0m \u001b[0msheet_by_index\u001b[0m \u001b[1;33m(\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m \u001b[1;33m.\u001b[0m \u001b[0masarray\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0msheet\u001b[0m \u001b[1;33m.\u001b[0m \u001b[0mrow_values\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0msheet\u001b[0m \u001b[1;33m.\u001b[0m \u001b[0mnrows\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\xlrd\\__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[1;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# a ZIP file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/fire_theft.xls'"
     ]
    }
   ],
   "source": [
    "#Lesson 3: Fire-theft problem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import xlrd\n",
    "DATA_FILE = \"data/fire_theft.xls\"\n",
    "# Step 1: read in data from the .xls file\n",
    "book = xlrd . open_workbook ( DATA_FILE , encoding_override = \"utf-8\")\n",
    "sheet = book . sheet_by_index ( 0)\n",
    "data = np . asarray ([ sheet . row_values ( i ) for i in range ( 1 , sheet . nrows )])\n",
    "n_samples = sheet . nrows - 1\n",
    "# Step 2: create placeholders for input X (number of fire) and label Y (number of theft)\n",
    "X = tf . placeholder ( tf . float32 , name = \"X\")\n",
    "Y = tf . placeholder ( tf . float32 , name = \"Y\")\n",
    "# Step 3: create weight and bias, initialized to 0\n",
    "w = tf . Variable ( 0.0 , name = \"weights\")\n",
    "b = tf . Variable ( 0.0 , name = \"bias\")\n",
    "# Step 4: construct model to predict Y (number of theft) from the number of fire\n",
    "Y_predicted = X * w + b\n",
    "# Step 5: use the square error as the loss function\n",
    "loss = tf . square ( Y - Y_predicted , name = \"loss\")\n",
    "# Step 6: using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf . train . GradientDescentOptimizer ( learning_rate = 0.001 ). minimize ( loss)\n",
    "with tf . Session () as sess:\n",
    "# Step 7: initialize the necessary variables, in this case, w and b\n",
    "    sess . run ( tf . global_variables_initializer ())\n",
    "    # Step 8: train the model\n",
    "    for i in range ( 100 ): # run 100 epochs\n",
    "        for x , y in data:\n",
    "    # Session runs train_op to minimize loss\n",
    "            sess . run ( optimizer , feed_dict ={ X : x , Y : y }) #run optimizerçš„æ—¶å€™ä¸éœ€è¦è¿”å›å€¼ï¼Œä½†run optimizerå°±ä¼šminimize lossï¼ŒåŒæ—¶å°±ä¼šæ”¹å˜wçš„å€¼\n",
    "    # Step 9: output the values of w and b\n",
    "    w_value , b_value = sess . run ([ w , b ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting /data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting /data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-33c10f1ac667>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mn_batches\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mX_batch\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mY_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMNIST\u001b[0m \u001b[1;33m.\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m.\u001b[0m \u001b[0mnext_batch\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#æŒ‰ç…§batchsizeè·å–batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0msess\u001b[0m \u001b[1;33m.\u001b[0m \u001b[0mrun\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mX_batch\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mY_batch\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#è¿™æ˜¯trainçš„è¿‡ç¨‹å¦‚æœæƒ³è·å–å•æ¬¡è®­ç»ƒçš„lossï¼Œå¯ä»¥åœ¨å‰é¢åŠ ä¸Š  _,l =\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;31m# average loss should be around 0.35 after 25 epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m# test the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Lesson 3: MNIST \n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow . examples . tutorials . mnist import input_data\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
    "MNIST = input_data . read_data_sets ( \"/data/mnist\" , one_hot = True )\n",
    "# Step 2: Define parameters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 25\n",
    "# Step 3: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9.\n",
    "# each label is one hot vector.\n",
    "X = tf . placeholder ( tf . float32 , [ batch_size , 784 ])#784æ˜¯å›¾ç‰‡å°ºå¯¸ï¼Œ10æ˜¯ç±»åˆ«æ•°ï¼Œæ˜¯å®šçš„ï¼Œbatch_sizeæ˜¯ä¹‹åèµ‹çš„\n",
    "Y = tf . placeholder ( tf . float32 , [ batch_size , 10 ])\n",
    "# Step 4: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf . Variable ( tf . random_normal ( shape =[ 784 , 10 ], stddev = 0.01 ), name = \"weights\")\n",
    "b = tf . Variable ( tf . zeros ([ 1 , 10 ]), name = \"bias\")\n",
    "# Step 5: predict Y from X and w, b\n",
    "# the model that returns probability distribution of possible label of the image\n",
    "# through the softmax layer\n",
    "# a batch_size x 10 tensor that represents the possibility of the digits\n",
    "logits = tf . matmul ( X , w ) + b #X@wå¾—åˆ°1*10è¡Œå‘é‡ï¼ŒYæ˜¯çœŸå®labelï¼Œlogitsæ˜¯é¢„æµ‹label\n",
    "# Step 6: define loss function\n",
    "# use softmax cross entropy with logits as the loss function\n",
    "# compute mean cross entropy, softmax is applied internally\n",
    "entropy = tf . nn . softmax_cross_entropy_with_logits ( labels = Y , logits = logits) #è°ƒç”¨å±æ€§å¯ä»¥ç›´æ¥ç”¨\n",
    "loss = tf . reduce_mean ( entropy) # computes the mean over examples in the batch lossçš„è®¡ç®—æ–¹æ³•åœ¨runä¹‹å‰å®šä¹‰å¥½ï¼Œä¸Šé¢æ±‚entropyæ˜¯ååŠ©lossçš„å®šä¹‰\n",
    "# Step 7: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize cost\n",
    "optimizer = tf . train . GradientDescentOptimizer ( learning_rate = learning_rate ). milimiZå`*`Inju	$ #äºRõ‘gpTmmizerçŸ€üsln,Mà€½õ¨åÑ}¸Êj×~çŠæÓüõ‚™g•¨Š~be¤a(´r)&@v`!Õv .$g<Mtà$×fíwiqtm7ıWhni|IAèyúerbo­À\`,!0(`
sith(tF >0m÷Siÿ~b !!aS óusr
Ø®:
 0& z#  òmö >¢su¨ ("idh}#­D/f,;$  (b¡ V_‚õ]B
aó =âkn¾%(rOLkYİ9/ <òƒL,g.°le/_%ÙaL"l-{ # BeôcX³qre'¦¹‚¦˜	Ü…Âà¼¢vKhAL\.2.+    "    for&9<*W"range ( î'²?sh’(©x a tramo0òh5$/Tnlm N_áhA`spieds\n",Š  â0"    $2!nêr _ yj bcngd ¬0njatcldc¦)\î2,
   0"e(¼" ¤ `b  X_b`tæH¤­ İnadah ="MFISĞ . tra!ê@`Oeh|Jmuch ( `A|ÂxßaiRd©´ ¦‘¥¥‹J¡ägÉ!izeèŒ·å–Bas¨gn¢
b  (" $$ 0"$!0&` 1Æ#{1`zun ([ ípx*}izmS äpLÊRsj]$fg%l^Giku ="Hº;lodqt#` ,`y ›0Y^b¥tcI })(#ˆ››GÀ®tQaao§š„É½‡ï©‹å¦‚æŒöÃ÷ ¾‘ÄÏ‡¥z•‡°¡˜¤-ñ»›¤š„MOsòï8ìí¿äº¥aœ(í‰ïm¢§Š ä¸Š  _,L ¹0Ll",
$ 0"# QVeàq-l \k3{ ch/ılDBâõ1i¶M1Ee‰1~qu	aVt¥h :´ o2Gfhs\n#,( $' tdsğ t|%gÎE},0d26	  " 
!±°änBetağmRn9|là"(0MASV`&!Ug3x!?*îEi_dxåMpÍuw0/ `a|bh_1iRE©L.",;$h8" 0!$Øş÷ñê_!^z2Mgu_ôbe$û!=¶0\G"
   à ‘`æ~p !1i(ö`nãe(øv?na´chs +zTn¢,
â  â" b0 `( "h×³{vã* âPY?Kádbl¤-"NIrT . ~g[q!dngY|^"atBa œ$"itaèWr+ze)|n¢,
    "        logits_batch = sess . run (logits, feed_dict ={ X : X_batch , Y : Y_batch }) #ç”¨åˆšæ‰æ±‚å‡ºæ¥çš„wå’Œbæ±‚test dataçš„labelï¼Œæ±‚å‡ºæ¥æ˜¯é•¿ä¸ºbatchsizeçš„å‘é‡\n",
    "        preds = tf . nn . softmax ( logits_batch)  \n",
    "        correct_preds = tf . equal ( tf . argmax ( preds , 1 ), tf . argmax ( Y_batch , 1 ))  #é•¿ä¸ºbatchsizeçš„true or falseå‘é‡\n",
    "        accuracy = tf . reduce_sum ( tf . cast ( correct_preds , tf . float32 )) #number of correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word2vec\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import tensorflow as tf\n",
    "\n",
    "def huber_loss(labels, predictions, delta=1.0):\n",
    "    residual = tf.abs(predictions - labels)\n",
    "    def f1(): return 0.5 * tf.square(residual)\n",
    "    def f2(): return delta * residual - 0.5 * tf.square(delta)\n",
    "    return tf.cond(residual < delta, f1, f2)\n",
    "\n",
    "def make_dir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "    \tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "# Parameters for downloading data\n",
    "DOWNLOAD_URL = 'http://mattmahoney.net/dc/'\n",
    "EXPECTED_BYTES = 31344016\n",
    "DATA_FOLDER = 'data/'\n",
    "FILE_NAME = 'text8.zip'\n",
    "\n",
    "def download(file_name, expected_bytes):\n",
    "    \"\"\" Download the dataset text8 if it's not already downloaded \"\"\"\n",
    "    file_path = DATA_FOLDER + file_name\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"Dataset ready\")\n",
    "        return file_path\n",
    "    file_name, _ = urllib.request.urlretrieve(DOWNLOAD_URL + file_name, file_path)\n",
    "    file_stat = os.stat(file_path)\n",
    "    if file_stat.st_size == expected_bytes:\n",
    "        print('Successfully downloaded the file', file_name)\n",
    "    else:\n",
    "        raise Exception('File ' + file_name +\n",
    "                        ' might be corrupted. You should try downloading it with a browser.')\n",
    "    return file_path\n",
    "\n",
    "def read_data(file_path):\n",
    "    \"\"\" Read data into a list of tokens \n",
    "    There should be 17,005,207 tokens\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(file_path) as f:\n",
    "        words = tf.compat.as_str(f.read(f.namelist()[0])).split() \n",
    "        # tf.compat.as_str() converts the input into the string\n",
    "    return words\n",
    "\n",
    "def build_vocab(words, vocab_size):\n",
    "    \"\"\" Build vocabulary of VOCAB_SIZE most frequent words \"\"\"\n",
    "    dictionary = dict()\n",
    "    count = [('UNK', -1)]\n",
    "    count.extend(Counter(words).most_common(vocab_size - 1))\n",
    "    index = 0\n",
    "    make_dir('processed')\n",
    "    with open('processed/vocab_1000.tsv', \"w\") as f:\n",
    "        for word, _ in count:\n",
    "            dictionary[word] = index\n",
    "            if index < 1000:\n",
    "                f.write(word + \"\\n\")\n",
    "            index += 1\n",
    "    index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, index_dictionary\n",
    "\n",
    "def convert_words_to_index(words, dictionary):\n",
    "    \"\"\" Replace each word in the dataset with its index in the dictionary \"\"\"\n",
    "    return [dictionary[word] if word in dictionary else 0 for word in words]\n",
    "\n",
    "def generate_sample(index_words, context_window_size):\n",
    "    \"\"\" Form training pairs according to the skip-gram model. \"\"\"\n",
    "    for index, center in enumerate(index_words):\n",
    "        context = random.randint(1, context_window_size)\n",
    "        # get a random target before the center word\n",
    "        for target in index_words[max(0, index - context): index]:\n",
    "            yield center, target\n",
    "        # get a random target after the center wrod\n",
    "        for target in index_words[index + 1: index + context + 1]:\n",
    "            yield center, target\n",
    "\n",
    "def get_batch(iterator, batch_size):\n",
    "    \"\"\" Group a numerical stream into batches and yield them as Numpy arrays. \"\"\"\n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1])\n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(iterator)\n",
    "        yield center_batch, target_batch\n",
    "\n",
    "def process_data(vocab_size, batch_size, skip_window):\n",
    "    file_path = download(FILE_NAME, EXPECTED_BYTES)\n",
    "    words = read_data(file_path)\n",
    "    dictionary, _ = build_vocab(words, vocab_size)\n",
    "    index_words = convert_words_to_index(words, dictionary)\n",
    "    del words # to save memory\n",
    "    single_gen = generate_sample(index_words, skip_window)\n",
    "    return get_batch(single_gen, batch_size)\n",
    "\n",
    "def get_index_vocab(vocab_size):\n",
    "    file_path = download(FILE_NAME, EXPECTED_BYTES)\n",
    "    words = read_data(file_path)\n",
    "    return build_vocab(words, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready\n",
      "<generator object get_batch at 0x00000000128D2BA0>\n",
      "[  568   687   687  7089  7089     1     1   248   248  5234  5234    11\n",
      "    11  1053  1053    28    28     1     1   321   321   249   249 44612\n",
      " 44612  2878  2878   793   793   187   187  5234  5234    12    12     6\n",
      "     6   201   201   603   603    11    11     1     1  1135  1135    20\n",
      "    20  2622  2622    26    26  8984  8984     3     3   280   280    32\n",
      "    32  4148  4148   142   142    60    60    26    26  6438  6438  4187\n",
      "  4187     2     2   154   154    33    33   363   363  5234  5234    37\n",
      "    37  1138  1138     7     7   448   448   345   345  1819  1819    20\n",
      "    20  4861  4861     1     1  6754  6754     2     2  7574  7574  1775\n",
      "  1775   567   567     1     1    94    94     1     1   248   248 11065\n",
      " 11065    12    12    52    52  7089  7089    90]\n",
      "[[  6.870000°0åi0pM\.+¬ c ¤  2 * =7¶XR  d8u)p2]}n",
 $ 0* 4`.8ù ´0pe#0]\n"¼
à.¤s¤* Z$06n 7q8´000e+(2=PL"¤(" ,! " $À=2ô8Q000g:±].),‚1` 0bj R! ·
0¨+p0 à«¤3]\n¢,Šb¬bd " [ 76°4<04Í*°h}|ê2,
 " (!!"¤[  1:`º$0 "0%#00Y\~ ,
 $ 4"0$ K2#uª334010´0a# Y|n¢<`"° "bQ (?® < "pp8d+¦"…$è-Æ$  à "@[¤£Q"r14¸0$0%#01Y\~ l
#¬!b""(Z  5.p30 °"e#13]\n",`"1 †!Û´(145300000U;ğkU\n2-J¬! $  p (`9n=8!4209t¯p9]\N"¬ :#"#'ar$¤2.¸01r”2´(t)01]\n¢,
bô pI > /,°;/r=s4 p#°#n $ p"  [  1.00000000e0 EFn$*$° d  ¢b[b02n00000000e+01\\ê&,"! ¤$ 2"S 7<091040`2m
6M\¾*lDà8"sdà*  1.0°00r00¥;0]\n"¬
 b    ¢0û!àä.9 20p°u
0\Üú"ì
`c 0("@_ ³,2[±° r00å#0PY\n ¬@!*  	 ¤Û bvl5v2£l¸ai44M^nb,
! ½ Ä.b[`(2.0¹0 20005+p]Ll" ² <#0  [  2.x7X°0pE+‘']Ll", 0" ! ¦ [ "t65R9§2´00u+p<]\f¢Œbbb ( " [ 07~sº 8q00D#0Y\n",
 `   " J"ä2+8'º00r00`+¤3]To",
   `b#¨Z0"1.7°00p(0%0¶MLì",ˆ  Â ¤b* Z$$'.)3p80000e*°¶]n",*  0 °(bL_8#52“$°"´8%%2»D\N&,
! ¤ ! ¦ [ 03n:6(1p810¡+0]Ll", (!   "&[01."0p8°°òui01\ë"‰
    0 bW  3.23´00s20e+03]\n",
      " [  6.00000000e+00]\n",
      " [  1.20000000e+01]\n",
      " [  2.01000000e+02]\n",
      " [  6.00000000e+00]\n",
      " [  6.03000000e+02]\n",
      " [  2.01000000e+02]\n",
      " [  1.10000000e+01]\n",
      " [  6.03000000e+02]\n",
      " [  1.00000000e+00]\n",
      " [  1.10000000e+01]\n",
      " [  1.13500000e+03]\n",
      " [  1.00000000e+00]\n",
      " [  2.00000000e+01]\n",
      " [  1.13500000e+03]\n",
      " [  2.62200000e+03]\n",
      " [  2.00000000e+01]\n",
      " [  2.60000000e+01]\n",
      " [  2.62200000e+03]\n",
      " [  8.98400000e+03]\n",
      " [  2.60000000e+01]\n",
      " [  3.00000000e+00]\n",
      " [  8.98400000e+03]\n",
      " [  2.80000000e+02]\n",
      " [  3.00000000e+00]\n",
      " [  3.20000000e+01]\n",
      " [  2.80000000e+02]\n",
      " [  4.14800000e+03]\n",
      " [  3.20000000e+01]\n",
      " [  1.42000000e+02]\n",
      " [  4.14800000e+03]\n",
      " [  6.00000000e+01]\n",
      " [  1.42000000e+02]\n",
      " [  2.60000000e+01]\n",
      " [  6.00000000e+01]\n",
      " [  6.43800000e+03]\n",
      " [  2.60000000e+01]\n",
      " [  4.18700000e+03]\n",
      " [  6.43800000e+03]\n",
      " [  2.00000000e+00]\n",
      " [  4.18700000e+03]\n",
      " [  1.54000000e+02]\n",
      " [  2.00000000e+00]\n",
      " [  3.30000000e+01]\n",
      " [  1.54000000e+02]\n",
      " [  3.63000000e+02]\n",
      " [  3.30000000e+01]\n",
      " [  5.23400000e+03]\n",
      " [  3.63000000e+02]\n",
      " [  3.70000000e+01]\n",
      " [  5.23400000e+03]\n",
      " [  1.13800000e+03]\n",
      " [  3.70000000e+01]\n",
      " [  7.00000000e+00]\n",
      " [  1.13800000e+03]\n",
      " [  4.48000000e+02]\n",
      " [  7.00000000e+00]\n",
      " [  3.45000000e+02]\n",
      " [  4.48000000e+02]\n",
      " [  1.81900000e+03]\n",
      " [  3.45000000e+02]\n",
      " [  2.00000000e+01]\n",
      " [  1.81900000e+03]\n",
      " [  4.86100000e+03]\n",
      " [  2.00000000e+01]\n",
      " [  1.00000000e+00]\n",
      " [  4.86100000e+03]\n",
      " [  6.75400000e+03]\n",
      " [  1.00000000e+00]\n",
      " [  2.00000000e+00]\n",
      " [  6.75400000e+03]\n",
      " [  7.57400000e+03]\n",
      " [  2.00000000e+00]\n",
      " [  1.77500000e+03]\n",
      " [  7.57400000e+03]\n",
      " [  5.67000000e+02]\n",
      " [  1.77500000e+03]\n",
      " [  1.00000000e+00]\n",
      " [  5.67000000e+02]\n",
      " [  9.40000000e+01]\n",
      " [  1.00000000e+00]\n",
      " [  1.00000000e+00]\n",
      " [  9.40000000e+01]\n",
      " [  2.48000000e+02]\n",
      " [  1.00000000e+00]\n",
      " [  1.10650000e+04]\n",
      " [  2.48000000e+02]\n",
      " [  1.20000000e+01]\n",
      " [  1.10650000e+04]\n",
      " [  5.20000000e+01]\n",
      " [  1.20000000e+01]\n",
      " [  7.08900000e+03]\n",
      " [  5.20000000e+01]\n",
      " [  9.00000000e+01]\n",
      " [  7.08900000e+03]]\n",
      "Average loss at step 1999: 114.1\n",
      "Average loss at step 3999:  53.1\n",
      "Average loss at step 5999:  33.3\n",
      "Average loss at step 7999:  23.3\n",
      "Average loss at step 9999:  17.9\n"
     ]
    }
   ],
   "source": [
    "\"\"\" The no frills implementation of word2vec skip-gram model using NCE loss.\n",
    "Author: Chip Huyen\n",
    "Prepared for the class CS 20SI: \"TensorFlow for Deep Learning Research\"\n",
    "cs20si.stanford.edu\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = 128 # dimension of the word embedding vectors\n",
    "SKIP_WINDOW = 1 # the context window\n",
    "NUM_SAMPLED = 64    # Number of negative examples to sample.\n",
    "LEARNING_RATE = 1.0\n",
    "NUM_TRAIN_STEPS = 10000\n",
    "SKIP_STEP = 2000 # how many steps to skip before reporting the loss\n",
    "\n",
    "\n",
    "def word2vec(batch_gen):\n",
    "    \"\"\" Build the graph for word2vec model and train it \"\"\"\n",
    "    # Step 1: define the placeholders for input and output\n",
    "    with tf.name_scope('data'):\n",
    "        center_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE], name='center_words')  #é•¿ä¸ºbatch_sizeçš„åˆ—è¡¨\n",
    "        target_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1], name='target_words')  #batch_size*1çš„çŸ©é˜µ\n",
    "\n",
    "    # Assemble this part of the graph on the CPU. You can change it to GPU if you have GPU\n",
    "    # Step 2: define weights. In word2vec, it's actually the weights that we care about\n",
    "\n",
    "    with tf.name_scope('embedding_matrix'):\n",
    "        embed_matrix = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], -1.0, 1.0), #-1,1æ˜¯maxvalueå’Œminvalueã€\n",
    "                            name='embed_matrix')\n",
    "\n",
    "    # Step 3: define the inference\n",
    "    with tf.name_scope('loss'):\n",
    "        embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embed')  #center_wordsä¸ºä¸€ä¸ªä¸€ç»´åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­çš„æ•°å­—ä»£è¡¨embed_matrixä¸­\n",
    "        #çš„è¡Œï¼Œembedç­‰äºembed_matrixä¸­è¿™äº›è¡Œæ„æˆçš„äºŒç»´çŸ©é˜µ\n",
    "\n",
    "        # Step 4: construct variables for NCE loss\n",
    "        nce_weight = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE],\n",
    "                                                    stddev=1.0 / (EMBED_SIZE ** 0.5)), \n",
    "                                                    name='nce_weight')\n",
    "        nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]), name='nce_bias')\n",
    "\n",
    "        # define loss function to be NCE loss function\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n",
    "                                            biases=nce_bias, \n",
    "                                            labels=target_words, \n",
    "                                            inputs=embed, \n",
    "                                            num_sampled=NUM_SAMPLED, \n",
    "                                            num_classes=VOCAB_SIZE), name='loss')\n",
    "\n",
    "    # Step 5: define optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        total_loss = 0.0 # we use this to calculate late average loss in the last SKIP_STEP steps\n",
    "        writer = tf.summary.FileWriter('./graphs/no_frills/', sess.graph)\n",
    "        for index in range(NUM_TRAIN_STEPS):\n",
    "            centers, targets = next(batch_gen) #batch_genæ˜¯æ•°æ®æ¥æº\n",
    "            loss_batch, _ = sess.run([loss, optimizer], \n",
    "                                    feed_dict={center_words: centers, target_words: targets})  #target_wordsæ˜¯label\n",
    "            total_loss += loss_batch\n",
    "            if (index + 1) % SKIP_STEP == 0:\n",
    "                print('Average loss at step {}: {:5.1f}'.format(index, total_loss / SKIP_STEP))\n",
    "                total_loss = 0.0\n",
    "        writer.close()\n",
    "\n",
    "def main():\n",
    "    batch_gen = process_data(VOCAB_SIZE, BATCH_SIZE, SKIP_WINDOW)\n",
    "    print(batch_gen)\n",
    "    word2vec(batch_gen)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#t-SNE virsulization\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "# obtain the embedding_matrix after youâ€™ve trained it\n",
    "final_embed_matrix = sess . run ( model . embed_matrix) #get handle\n",
    "# create a variable to hold your embeddings. It has to be a variable. Constants\n",
    "# donâ€™t work. You also canâ€™t just use the embed_matrix we defined earlier for our model. Why\n",
    "# is that so? I donâ€™t know. I get the 500 most popular words.\n",
    "embedding_var = tf.Variable(final_embed_matrix[:500], name='embedding')\n",
    "sess.run(embedding_var.initializer)\n",
    "config = projector.ProjectorConfig()\n",
    "summary_writer = tf.summary.FileWriter(LOGDIR)\n",
    "# add embeddings to config\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "# link the embeddings to their metadata file. In this case, the file that contains\n",
    "# the 500 most popular words in our vocabulary\n",
    "embedding.metadata_path = LOGDIR + '/vocab_500.tsv'\n",
    "# save a configuration file that TensorBoard will read during startup\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "# save our embedding\n",
    "saver_embed = tf.train.Saver([embedding_var])\n",
    "saver_embed.save(sess, LOGDIR + '/skip-gram.ckpt', 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
